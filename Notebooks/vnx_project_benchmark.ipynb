{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pattern Matching Kernel Test\n",
    "\n",
    "This notebook tests the pattern matching kernel that searches for malicious patterns in UDP packets.\n",
    "\n",
    "The kernel:\n",
    "- Receives UDP packets from the network\n",
    "- Searches for 256 predefined patterns in a sliding window\n",
    "- Returns match results: each byte in the output indicates if a pattern was found at that position (pattern_id+1) or no match (0)\n",
    "\n",
    "Test setup:\n",
    "1. Program FPGA with the pattern matching design\n",
    "2. Configure network layer and socket table\n",
    "3. Send test data containing known patterns from host to FPGA\n",
    "4. Verify FPGA correctly identifies pattern locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing 'HOT Reset' on '0000:02:00.1'\n",
      "Are you sure you wish to proceed? [Y/n]: Y (Force override)\n",
      "Successfully reset Device[0000:02:00.1]\n"
     ]
    }
   ],
   "source": [
    "#Uncomment the next line and run to reset the FPGA if it is not taking programming or otherwise misbehaving\n",
    "!xbutil reset --device 0000:02:00.1 --force"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pynq\n",
    "import numpy as np\n",
    "import vnx_utils\n",
    "import os\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Available Devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0) xilinx_u55c_gen3x16_xdma_base_3\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(pynq.Device.devices)):\n",
    "    print(\"{}) {}\".format(i, pynq.Device.devices[i].name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Program FPGA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available IPs: ['krnl_mm2s_0', 'krnl_s2mm_0', 'stream_throughput_0']\n",
      "Using Memory Bank: <pynq.pl_server.xrt_device.XrtMemory object at 0x7fdc5185fdf0>\n"
     ]
    }
   ],
   "source": [
    "ol = pynq.Overlay(\"../benchmark_project.intf0.xilinx_u55c_gen3x16_xdma_3_202210_1/vnx_benchmark_project_if0.xclbin\")\n",
    "\n",
    "# Print available IPs\n",
    "print(\"Available IPs:\", list(ol.ip_dict.keys()))\n",
    "\n",
    "# Chain: MM2S -> Throughput -> Project Kernel -> S2MM\n",
    "krnl_mm2s = ol.krnl_mm2s_0\n",
    "krnl_s2mm = ol.krnl_s2mm_0\n",
    "stream_throughput = ol.stream_throughput_0\n",
    "\n",
    "mem_bank = ol.HBM0\n",
    "    \n",
    "print(f\"Using Memory Bank: {mem_bank}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Test Patterns\n",
    "\n",
    "We'll use some of the actual patterns from patterns.h for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration: DWIDTH=32 bits (4 bytes/beat)\n",
      "Output View: <class 'numpy.uint32'>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "KERNEL_DWIDTH_BITS = 32 \n",
    "\n",
    "# Derived Constants\n",
    "BYTES_PER_BEAT = KERNEL_DWIDTH_BITS // 8\n",
    "\n",
    "# Map DWIDTH to Numpy Data Types for viewing the output\n",
    "DTYPE_MAP = {\n",
    "    8:  np.uint8,\n",
    "    16: np.uint16,\n",
    "    32: np.uint32,\n",
    "    64: np.uint64\n",
    "}\n",
    "\n",
    "if KERNEL_DWIDTH_BITS not in DTYPE_MAP:\n",
    "    raise ValueError(f\"Unsupported DWIDTH: {KERNEL_DWIDTH_BITS}. Please use 8, 16, 32, or 64.\")\n",
    "\n",
    "KERNEL_DTYPE = DTYPE_MAP[KERNEL_DWIDTH_BITS]\n",
    "\n",
    "print(f\"Configuration: DWIDTH={KERNEL_DWIDTH_BITS} bits ({BYTES_PER_BEAT} bytes/beat)\")\n",
    "print(f\"Output View: {KERNEL_DTYPE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully parsed 2662 patterns.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def parse_patterns_header(file_path):\n",
    "    \"\"\"\n",
    "    Parses C++ header 'patterns.h' to extract pattern data and IDs.\n",
    "    Returns: { global_pattern_id : byte_array }\n",
    "    \"\"\"\n",
    "    patterns_db = {}\n",
    "    \n",
    "    # Resolve absolute path if needed, or use relative\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"Error: {file_path} not found.\")\n",
    "        print(\"Please check the path to patterns.h relative to this notebook.\")\n",
    "        return {}\n",
    "\n",
    "    with open(file_path, 'r') as f:\n",
    "        content = f.read()\n",
    "\n",
    "\n",
    "    re_data = re.search(r'const unsigned char PATTERN_DATA\\[\\d+\\]\\[\\d+\\]\\s*=\\s*\\{(.*?)\\};', content, re.DOTALL)\n",
    "    re_len = re.search(r'const int PATTERN_LENGTHS\\[\\d+\\]\\[\\d+\\]\\s*=\\s*\\{(.*?)\\};', content, re.DOTALL)\n",
    "    re_counts = re.search(r'const int NUM_PATTERNS_MATRIX\\[\\d+\\]\\s*=\\s*\\{([^}]+)\\};', content)\n",
    "    re_offsets = re.search(r'const int PATTERN_OFFSETS\\[\\d+\\]\\[\\d+\\]\\s*=\\s*\\{(.*?)\\};', content, re.DOTALL)\n",
    "\n",
    "    if not (re_data and re_len and re_counts and re_offsets):\n",
    "        print(\"Error: Could not parse array structures in patterns.h. Check file format.\")\n",
    "        return {}\n",
    "\n",
    "    # Helper to clean and split C-style array strings\n",
    "    def parse_c_array(raw_str):\n",
    "        # Split by the closing brace of each row '},'\n",
    "        rows = raw_str.split('},')\n",
    "        matrix = []\n",
    "        for row in rows:\n",
    "            clean_row = row.replace('{', '').replace('}', '').strip()\n",
    "            if clean_row:\n",
    "                items = []\n",
    "                for x in clean_row.split(','):\n",
    "                    x = x.strip()\n",
    "                    if not x: continue\n",
    "                    try:\n",
    "                        val = int(x, 16) if x.startswith('0x') else int(x)\n",
    "                        items.append(val)\n",
    "                    except ValueError:\n",
    "                        continue\n",
    "                matrix.append(items)\n",
    "        return matrix\n",
    "\n",
    "    data_matrix = parse_c_array(re_data.group(1))\n",
    "    len_matrix = parse_c_array(re_len.group(1))\n",
    "    offset_matrix = parse_c_array(re_offsets.group(1))\n",
    "    counts = [int(x.strip()) for x in re_counts.group(1).split(',')]\n",
    "\n",
    "    # Calculate Global ID Offsets (cumulative sum of counts)\n",
    "    global_id_offsets = [0] * len(counts)\n",
    "    for i in range(1, len(counts)):\n",
    "        global_id_offsets[i] = global_id_offsets[i-1] + counts[i-1]\n",
    "\n",
    "    # Build Dictionary\n",
    "    for n in range(min(len(counts), len(data_matrix))):\n",
    "        num_pats = counts[n]\n",
    "        for p in range(num_pats):\n",
    "            # Safety check for indices to prevent out-of-bounds\n",
    "            if n >= len(len_matrix) or p >= len(len_matrix[n]): continue\n",
    "            if n >= len(offset_matrix) or p >= len(offset_matrix[n]): continue\n",
    "            \n",
    "            p_len = len_matrix[n][p]\n",
    "            p_start = offset_matrix[n][p]\n",
    "            \n",
    "            # Verify data bounds\n",
    "            if p_len > 0 and (p_start + p_len) <= len(data_matrix[n]):\n",
    "                # Extract the pattern bytes\n",
    "                pat_bytes = data_matrix[n][p_start : p_start + p_len]\n",
    "                global_id = p + global_id_offsets[n]\n",
    "                patterns_db[global_id] = pat_bytes\n",
    "\n",
    "    return patterns_db\n",
    "\n",
    "# Load patterns\n",
    "patterns_path = \"../Project_kernels_HLS/src/patterns.h\" \n",
    "patterns_map = parse_patterns_header(patterns_path)\n",
    "print(f\"Successfully parsed {len(patterns_map)} patterns.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Test Data with Patterns\n",
    "\n",
    "This cell generates random traffic and injects known patterns to verify the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating data for 32-bit Kernel...\n"
     ]
    }
   ],
   "source": [
    "def generate_parametric_stream(total_bytes, patterns, num_injections=50):\n",
    "    # Initialize with Zeros\n",
    "    data = np.zeros(total_bytes, dtype=np.uint8) \n",
    "    expected = {}\n",
    "    \n",
    "    if not patterns: return data, expected\n",
    "\n",
    "    keys = list(patterns.keys())\n",
    "    \n",
    "    # Start offset\n",
    "    current_idx = 128 \n",
    "    \n",
    "    for _ in range(num_injections):\n",
    "        if current_idx >= (total_bytes - 512): break\n",
    "        \n",
    "        pid = np.random.choice(keys)\n",
    "        pat = patterns[pid]\n",
    "        p_len = len(pat)\n",
    "        \n",
    "        # Inject Pattern\n",
    "        data[current_idx : current_idx + p_len] = pat\n",
    "        \n",
    "        # --- PARAMETRIC BEAT CALCULATION ---\n",
    "        end_idx = current_idx + p_len - 1\n",
    "        beat = end_idx // BYTES_PER_BEAT\n",
    "        \n",
    "        expected[beat] = pid\n",
    "        \n",
    "        # Add padding (aligned to 64 bytes to be safe)\n",
    "        current_idx += p_len + 64 \n",
    "        \n",
    "    return data, expected\n",
    "\n",
    "# Setup\n",
    "N_RATE = 1000\n",
    "N_SAMPLES = 100\n",
    "STREAM_SIZE = 64 * N_RATE * (N_SAMPLES + 1)\n",
    "\n",
    "print(f\"Generating data for {KERNEL_DWIDTH_BITS}-bit Kernel...\")\n",
    "input_data, expected_map = generate_parametric_stream(STREAM_SIZE, patterns_map, num_injections=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Buffer Allocation\n",
    "\n",
    "This cell allocates the memory on the FPGA card."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocating buffers...\n",
      "Populating input data...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Allocate Buffers in FPGA Memory\n",
    "print(\"Allocating buffers...\")\n",
    "input_buffer = pynq.allocate(shape=(STREAM_SIZE,), dtype=np.uint8)\n",
    "output_buffer = pynq.allocate(shape=(STREAM_SIZE,), dtype=np.uint8)\n",
    "\n",
    "# Throughput Monitor Struct\n",
    "sample_t = np.dtype([(\"cycles\", \"u4\"), (\"bytes\", \"u4\"), (\"ready_not_valid\", \"u4\"), (\"valid_not_ready\", \"u4\")])\n",
    "perf_buf = pynq.allocate((N_SAMPLES,), dtype=sample_t, target=mem_bank)\n",
    "\n",
    "print(\"Populating input data...\")\n",
    "# Copy the test_data created in the previous cell into the PYNQ buffer\n",
    "input_buffer[:] = input_data\n",
    "\n",
    "# Sync data to the device (flush cache)\n",
    "input_buffer.sync_to_device()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution\n",
    "Run the kernels and the throughput monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting kernels...\n",
      "Waiting for completion...\n",
      "Kernels finished.\n",
      "Received 6464000 bytes.\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting kernels...\")\n",
    "\n",
    "dest_id = 0 \n",
    "\n",
    "perf_w = stream_throughput.start(perf_buf, N_SAMPLES, N_RATE)\n",
    "\n",
    "s2mm_handle = ol.krnl_s2mm_0.start(output_buffer, STREAM_SIZE)\n",
    "\n",
    "mm2s_handle = ol.krnl_mm2s_0.start(input_buffer, STREAM_SIZE, dest_id)\n",
    "\n",
    "print(\"Waiting for completion...\")\n",
    "mm2s_handle.wait()\n",
    "s2mm_handle.wait()\n",
    "perf_w.wait()\n",
    "\n",
    "print(\"Kernels finished.\")\n",
    "\n",
    "# Retrieve Results\n",
    "output_buffer.sync_from_device()\n",
    "\n",
    "results = np.array(output_buffer)\n",
    "\n",
    "print(f\"Received {len(results)} bytes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Performance ---\n",
      "Transferred: 4032 bytes\n",
      "Cycles:      999\n",
      "Throughput:  9.686 Gbps\n",
      "\n",
      "--- Verification ---\n",
      "Total Injections: 100\n",
      "Matches Found:    100\n",
      "Misses:           0\n",
      "TEST PASSED!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Retrieve results\n",
    "perf_buf.sync_from_device()\n",
    "output_buffer.sync_from_device()\n",
    "output_ids = output_buffer.view(KERNEL_DTYPE)\n",
    "\n",
    "# --- Throughput Calculation ---\n",
    "total_bytes = perf_buf['bytes'][-1]\n",
    "total_cycles = perf_buf['cycles'][-1]\n",
    "freq_mhz = 300\n",
    "\n",
    "duration = total_cycles / (freq_mhz * 1e6)\n",
    "throughput = (total_bytes * 8) / (duration * 1e9)\n",
    "\n",
    "print(\"\\n--- Performance ---\")\n",
    "print(f\"Transferred: {total_bytes} bytes\")\n",
    "print(f\"Cycles:      {total_cycles}\")\n",
    "print(f\"Throughput:  {throughput:.3f} Gbps\")\n",
    "\n",
    "# --- Pattern Matching Verification ---\n",
    "print(\"\\n--- Verification ---\")\n",
    "matches = 0\n",
    "misses = 0\n",
    "\n",
    "for beat, exp_id in expected_map.items():\n",
    "    # Check a small window around the expected beat to account for pipeline latency\n",
    "    found = False\n",
    "    window = 10 # Check beat, beat+1...\n",
    "    \n",
    "    for w in range(window):\n",
    "        if (beat + w) < len(output_ids):\n",
    "            # Check if the output ID matches the expected Pattern ID\n",
    "            if output_ids[beat + w] == exp_id:\n",
    "                found = True\n",
    "                break\n",
    "    \n",
    "    if found:\n",
    "        matches += 1\n",
    "    else:\n",
    "        misses += 1\n",
    "\n",
    "print(f\"Total Injections: {len(expected_map)}\")\n",
    "print(f\"Matches Found:    {matches}\")\n",
    "print(f\"Misses:           {misses}\")\n",
    "\n",
    "if misses == 0 and matches > 0:\n",
    "    print(\"TEST PASSED!\")\n",
    "else:\n",
    "    print(\"TEST FAILED.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def save_debug_report(filename, expected_map, output_buffer):\n",
    "    print(f\"Generating debug report: {filename} ...\")\n",
    "    \n",
    "    # 1. Prepare Data\n",
    "    output_buffer.sync_from_device()\n",
    "    \n",
    "    output_data = output_buffer.view(np.uint16)\n",
    "    \n",
    "    with open(filename, \"w\") as f:\n",
    "        # --- SECTION 1: EXPECTED VS ACTUAL ---\n",
    "        f.write(\"=======================================================\\n\")\n",
    "        f.write(\"SECTION 1: VERIFICATION (Expected vs Actual)\\n\")\n",
    "        f.write(\"=======================================================\\n\")\n",
    "        f.write(f\"{'BEAT':<12} | {'EXPECTED ID':<12} | {'ACTUAL ID':<12} | {'STATUS'}\\n\")\n",
    "        f.write(\"-\" * 60 + \"\\n\")\n",
    "        \n",
    "        matches = 0\n",
    "        misses = 0\n",
    "        \n",
    "        # Sort by beat to keep it chronological\n",
    "        for beat, exp_id in sorted(expected_map.items()):\n",
    "            # Safety check for bounds\n",
    "            if beat < len(output_data):\n",
    "                act_id = output_data[beat]\n",
    "            else:\n",
    "                act_id = -1 # Out of bounds\n",
    "            \n",
    "            # Check for exact match\n",
    "            status = \"MATCH\" if act_id == exp_id else \"MISS\"\n",
    "            \n",
    "            # Check for near-miss (shifted by +/- 4 beats)\n",
    "            if status == \"MISS\":\n",
    "                for offset in range(-4, 5):\n",
    "                    check_idx = beat + offset\n",
    "                    if 0 <= check_idx < len(output_data):\n",
    "                        if output_data[check_idx] == exp_id:\n",
    "                            status = f\"SHIFTED ({offset:+d})\"\n",
    "                            break\n",
    "            \n",
    "            if status == \"MATCH\": \n",
    "                matches += 1\n",
    "            else: \n",
    "                misses += 1\n",
    "                \n",
    "            f.write(f\"{beat:<12} | {exp_id:<12} | {act_id:<12} | {status}\\n\")\n",
    "            \n",
    "        f.write(\"-\" * 60 + \"\\n\")\n",
    "        f.write(f\"SUMMARY: Matches: {matches}, Misses: {misses}, Total: {len(expected_map)}\\n\\n\\n\")\n",
    "\n",
    "        # --- SECTION 2: RAW HARDWARE DETECTIONS ---\n",
    "        f.write(\"=======================================================\\n\")\n",
    "        f.write(\"SECTION 2: ALL HARDWARE DETECTIONS (Non-zero outputs)\\n\")\n",
    "        f.write(\"=======================================================\\n\")\n",
    "        f.write(f\"{'BEAT':<12} | {'DETECTED ID'}\\n\")\n",
    "        f.write(\"-\" * 30 + \"\\n\")\n",
    "        \n",
    "        # Scan entire buffer for any non-zero value\n",
    "        hw_detections = np.nonzero(output_data)[0]\n",
    "        \n",
    "        if len(hw_detections) == 0:\n",
    "            f.write(\"No patterns detected (Output is all zeros).\\n\")\n",
    "        else:\n",
    "            for beat in hw_detections:\n",
    "                val = output_data[beat]\n",
    "                f.write(f\"{beat:<12} | {val}\\n\")\n",
    "                \n",
    "    print(f\"Report saved. Open '{filename}' to analyze.\")\n",
    "\n",
    "# Run the export (using the correct buffer variable 'output_buffer')\n",
    "save_debug_report(\"debug_results.txt\", expected_map, output_buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resources freed\n"
     ]
    }
   ],
   "source": [
    "del output_buffer\n",
    "del input_buffer\n",
    "del perf_buf\n",
    "ol.free()\n",
    "print(\"Resources freed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing 'HOT Reset' on '0000:02:00.1'\n",
      "Are you sure you wish to proceed? [Y/n]: Y (Force override)\n"
     ]
    }
   ],
   "source": [
    "#  Reset FPGA\n",
    "!xbutil reset --device 0000:02:00.1 --force"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
